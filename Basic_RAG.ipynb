{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4255ff-a373-45ae-bb03-b235eb210529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data\" \n",
    "\n",
    "# Document LOADER\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "# Split Documents into Chunks\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# DATABANK\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=get_embedding_function()\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents.\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"ðŸ‘‰ Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        \n",
    "    else:\n",
    "        print(\"âœ… No new documents to add\")\n",
    "\n",
    "\n",
    "def calculate_chunk_ids(chunks):\n",
    "\n",
    "    # This will create IDs like \"data/SILOP.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8acaf423-21eb-4cb6-84bf-5a31d5bf3726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 0\n",
      "ðŸ‘‰ Adding new documents: 428\n"
     ]
    }
   ],
   "source": [
    "# Create (or update) the data store.\n",
    "\n",
    "#print(\"âœ¨ Clearing Database\")\n",
    "#clear_database()\n",
    "\n",
    "\n",
    "documents = load_documents()\n",
    "chunks = split_documents(documents)\n",
    "add_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61460dab-f657-47d8-809d-f85ec87138e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "\n",
    "def query_rag(query_text: str):\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    model = OllamaLLM(model=\"llama3.2\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d20814c-1599-43b6-b886-e1d22e039477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: According to the text, the key features of SILOP (Semantic Image Labeling using Object Perimeters) are:\n",
      "\n",
      "1. An additional step between CAM (Class Activation Mapping) and pixel similarity-based refinement that improves prediction quality.\n",
      "2. The incorporation of a PerimeterFit module, which can be used with any conventional CAM framework and existing WSSS pipelines.\n",
      "3. Improved performance on certain classes, such as boat, person, and train, with gains of 4.9%, 3.7%, and 3.5% over PuzzleCAM, respectively.\n",
      "4. Better performance in removing false positives and achieving more mIoU (mean Intersection over Union) points when successfully refining the segmentation mask.\n",
      "\n",
      "Overall, SILOP appears to be an automated framework for semantic segmentation that leverages image-level labels based on object perimeters to improve prediction quality and robustness.\n",
      "Sources: ['data\\\\SILOP.pdf:1:2', 'data\\\\SILOP.pdf:6:4', 'data\\\\SILOP.pdf:6:3', 'data\\\\SILOP.pdf:0:0', 'data\\\\SILOP.pdf:2:3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the text, the key features of SILOP (Semantic Image Labeling using Object Perimeters) are:\\n\\n1. An additional step between CAM (Class Activation Mapping) and pixel similarity-based refinement that improves prediction quality.\\n2. The incorporation of a PerimeterFit module, which can be used with any conventional CAM framework and existing WSSS pipelines.\\n3. Improved performance on certain classes, such as boat, person, and train, with gains of 4.9%, 3.7%, and 3.5% over PuzzleCAM, respectively.\\n4. Better performance in removing false positives and achieving more mIoU (mean Intersection over Union) points when successfully refining the segmentation mask.\\n\\nOverall, SILOP appears to be an automated framework for semantic segmentation that leverages image-level labels based on object perimeters to improve prediction quality and robustness.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "query_text = \"Can you explain the key features of SILOP?\"\n",
    "query_rag(query_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
